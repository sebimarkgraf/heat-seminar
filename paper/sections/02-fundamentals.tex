\section{Fundamentals}
\label{sec:fundamentals}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/schematic-lcz.png}
  \caption{The LCZ classes as schematics on the left and matching of classes for a city in the centre. Image taken from \cite{zhu_so2sat_2019}.}\label{fig:lcz_classes}
\end{figure*}


\subsection{Distributed Array Computation}
\label{subsec:distributed_array_computation}
Many scientific fields use \gls{numpy} arrays as a basis for their calculation. Due to their implementation in the C language and their well designed
Python interface, they are fast to use and easy to understand. But, \gls{numpy} arrays are only implemented for single node use.
Multiple approaches to allow the use of \gls{numpy} arrays on multiple nodes have emerged.
Some of them like \cite{bauer_legate_2019} add \gls{GPU} support as well, to leverage the speedup for some of the operations.

The hardest part in distributed array computing lies in the distribution of the data. All implementations provide
either the option to specify how the data should be distributed or even try to estimate the best distribution \cite{huang_spartan_nodate}.
But all of them have in common, that the distribution of the data itself is encapsulated and hidden from the user.
This allows ease of use, but can lead to drops in performance when the user executes non optimal operations.


\subsection{MPI}
\label{subec:mpi}
\gls{MPI} is a message-passing standard published from the Message-Passing Interface Forum. It is currently available in version 3.1 \cite{message_passing_interface_forum_mpi_2015}.
\enquote{The goal of the Message-Passing-Interface, simply stated, is to develop a widely used standard for writing message-passing programs}\cite{message_passing_interface_forum_mpi_2015}.
With this goal \gls{MPI} is today often used in \gls{HPC}, especially in Grid Computing. The standard interface allows to pass results or data between different processes or nodes.
Due to \gls{MPI} being a standard multiple implementations exist, which support different versions of the standard.
Some of these implementations OpenMPI \cite{noauthor_open_nodate} and Intel MPI \cite{noauthor_intel_nodate}, which are available on our compute ressources.
\gls{MPI} has some limitations that should be remembered when using it with big data sets. The most important limitation is the maximum message size, due to the size
being specified as an integer. Therfore, the maximum number of values that can be passed is \(2^{31} - 1\). Bigger messages need to be split in multiple messages or workarounds need to be implemented.

\subsection{HEAT}
\label{subsec:heat}
\gls{HeAT} is introduced in \cite{krajsek_helmholtz_nodate}. \gls{HeAT} aims to be a mathemtical library for distributed computations.
It conforms to a \gls{numpy} \cite{noauthor_numpy_nodate} interface and makes all distributed calculations transparent. To move from executing \gls{HeAT} scripts single threaded to executing them parallel
or distributed one just needs to change the execution command to \gls{MPI}.
This is due to \gls{HeAT} wrapping \gls{PyTorch} as implementation with the correct \gls{MPI} synchronization to distribute the calculation.

To perform the calculation distributed correctly \gls{HeAT} adds a new data structure: the DNDArray. Users of other libraries
recognize the general interface of Numpy arrays or PyTorch Tensors, but extended by a split parameter.
The split parameter allows to control the axis, which the data is distributed across.
The developers decided to allow only 1 split axis to keep the development effort manageable.

Although, the distribution is transparent besides the split parameter several pitfalls exist.
When reshaping or splicing the dataset one has to potentially rebalance the dataset. Additionally, this could incurr communication effort
to redistribute the dataset across the nodes.
Another pitfalls especially for developers comming from the distributed programming, lies in all processes needing to call all \gls{HeAT} methods.
While in traditional \gls{MPI} one discriminates between the processes based on the rank, in \gls{HeAT} all nodes need to access all functions and DNDArrays or the
programm could lockup.

Lastly, \gls{HeAT} provides the load utility for distributed loading of datasets. Through the usage of all nodes at the same time \gls{IO} is maximized and the data
is splitted already during loading. Therefore, calculations can start right away without the need to distribute the data.
\cite{krajsek_helmholtz_nodate}


\subsection{Local Climate Zone Classification}
\label{subsec:local_climate_zone_classification}

\glspl{LCZ} are a scheme formally proposed by \citeauthor{stewart_local_2012} in \cite{stewart_local_2012}. The scheme classifies areas based on fabric, land cover, structure
and metabolism into one of 17 \gls{LCZ}s. \cite{xue_applications_2020}
The \gls{LCZ}s are decided by 4 components:
\begin{enumerate}
  \item Height of roughness features
  \item Packing of roughness features
  \item Surface cover around roughness features
  \item Thermal admittance of materials
\end{enumerate}
The different classes can be seen in \cref{fig:lcz_classes}.
The \glspl{LCZ} were introduced to get more explainable and general divisions of the suburban ares. They are complex and diverse which make temperature and other climate analysis harder
to understand and explain without a system. According to \citeauthor{stewart_local_2012}, the system fullfills all criteria of a logical local system as defined in \cite{grigg_logic_1965}.
Their work adds guidelines on how to classify an area correctly into their zones and how to use them afterwards.


\subsection{SO2Sat}
\label{subsec:so2sat}
\citeauthor{zhu_so2sat_2019} presented the SO2Sat dataset in \cite{zhu_so2sat_2019}. They describe SO2Sat as a \enquote{valuable benchmark dataset \textelp{}, which consists of local climate zone (LCZ) labels of about half a million \textelp{} image patches}.
These images are taken from the ESA satellites Sentinel-1 and Sentinel-2. Each image is 32 by 32 pixels and contains 8 channels for Sentinel-1 and 10 channels for Sentinel-2.
After preprocessing these images they were given to domain experts for labelling which follwed a \enquote{carefully designed labelling work flow} \cite{zhu_so2sat_2019}.
Through the careful work the dataset achieved a \enquote{overall confidence of 85\%} \cite{zhu_so2sat_2019}.
The labels consist of integers representing the 17 different \glspl{LCZ}. 52 cities are part of the dataset.

\subsection{Spectral Clustering}
\label{subsec:spectral_clustering}

\begin{algorithm}[b]
  \SetKwData{Laplace}{\(L\)}\SetKwData{Adj}{\(W\)}
  \KwData{Similariy matrix \(S\), number \(k\) of clusters}
  \KwResult{Clusters \(A_1, \ldots, A_k\) }
  Construct a similarity graph\;
  \Adj \(\leftarrow\) weighted adjacency matrix\;
  \Laplace \(\leftarrow\) computeLaplacian(\Adj)\;
  \(u_1, \ldots, u_k \leftarrow\) computeEigenvectors(L)\;
  \(U\) \(\leftarrow\) Matrix with columns \(u_1, \ldots, u_k\)\;
  \ForEach(){\(i = 1, \ldots, n\)}{
    \(y_i\) \(\leftarrow\) vector corresponding to the \(i\)-th row of \(U\)
  }
\(C_1, \ldots, C_k \leftarrow\) kMeans(\(y_1, \ldots, y_n\))\;

  \caption{Basic Spectral Clustering}\label{alg:basic_spectral}
 \end{algorithm}

\enquote{Spectral Clustering is the process of partitioning data samples into
\(k\) groups based on graph theory} \cite{krajsek_helmholtz_nodate}. Therefore,
to derive the formulation of Spectral Clustering we introduce the undirected graph \(G=(V, E)\).
We consider the graph as weighted by assigning each edge an weight \(w_{ij}\). As the graph
is unidrectional, the weight is identical for \(w_{ij} = w_{ji} \).
% Degree Matrix
We define the degree of a vertex \(v_i \in V\) by iterating over all vertices \(v_j \in V\) that are connected to \(v_i\).
Therefore, the weight of the edge is positive:
\[d_i = \sum_{j=1}^n w_{ij}\]
Using the degree defined before, we define the degree matrix \(D\) as the diagonal matrix with the degrees \(d_1, \ldots, d_n\) on the diagonal.
\cite{von_luxburg_tutorial_2007}
% Weights Matrix
For 2 subsets of indices \(A, B\) we define the weights matrix \cite{von_luxburg_tutorial_2007}:
\[W(A, B) := \sum_{i \in A, j \in B} w_{ij}\]


There are several possible ways to construct such a similarity graph from two datasets.
The following three are used often for spectral clustering:
\begin{itemize}
  \item Fully connected
  \item k-nearest neighbour
  \item \(\epsilon\)-neighbourhood
\end{itemize}
The fully connected graph considers the similarity between all samples. The graph should model the local neighbours and is therefore only useful when the distance function
models the width of the neighbourhood.
The k-nearest neighbour graph can be implemented in two ways. If we just use the defintion of the k-nearest neighbours we would receive an directed graph.
To convert this into a undirected graph we can decide between connecting both vertices if one of them is a k-nearest neighbour of the other or connecting them if both vertices are nearest neighbours of each other.
Last, the \(\epsilon\)-neighbourhood graph only connects vertices that are closer than \(\epsilon\).
Since, this option already models the neighbourhood directly it usually is considered as an unweighted graph. \cite{von_luxburg_tutorial_2007}
According to \cite{von_luxburg_tutorial_2007} there is no theoretical work on the choosing of one method over
another.

Using the constructed similarity graph we need to calculate the spectrum of the similarity matrix.
This is done by constructing the laplacian defined via
\[L = W - D\]
using \(W, D\) as defined earlier.

The Lanczos algorithm \cite{lanczos_iteration_1950} is used to calculate the eigenvalues and eigenvectors from the laplacian.
When using the \(k\) smallest eigenvalues and the korresponding eigenvectors \(e_1, \ldots, e_k\) we can embed the dataset.

Now a clustering can be performed in the smaller embedding space of the eigenbasis using another algorithm, e.g. KMeans.
The overall algorithm can be found in algorithm \ref{alg:basic_spectral}.

\begin{figure}
  \includegraphics{images/spectral_example.png}
  \caption{2 Half-Moon example for clustering performance of spectral clustering. Taken from \cite{noauthor_23_2020}.}
  \label{fig:spectral_example}
\end{figure}

One of the most common examples for spectral clustering can be seen in \cref{fig:spectral_example}. Spectral Clustering is able to
separate the nonlinear datasets. This is often compared with KMeans which fails to correctly recognize the different clusters when using the euclidean distance.

\subsection{Distance Metrics}
\label{subsec:distance_metrics}
For the spectral clustering the distance between samples needs to be measured. Depending on the data different ways to measure the distance are implemented in \gls{HeAT}.

\subsubsection{Euclidean Distance}
The euclidean distance is a well known way to measure the distance. The euclidean distance of two points \(X = (x_1, \ldots, x_n ), X' = (x_1', \ldots, x_n')\) is commonly defined  as:
\[
  d\left(X, X'\right) = \sqrt{{(x_1' - x_1)}^2 + \ldots + {(x_n' - x_n)}^2}
\]
It is fast and easy to calculate, but it allows only for linear separatation with most algorithms.

\subsubsection{Radial Basis Function Kernel}
The RBF kernel is a kernel that wraps the euclidean distance \(d\) \cite{vert_primer_2004}:
\[
  K\left(X, X'\right) = \exp(-\gamma d(X, X'))
\]
The RBF kernel is a measure of similarity and augments the effects of the euclidean distance due to the exponential function.
It allows to seperate non-linear data with many algorithms.


\subsection{Clustering Evaluation Metrics}
\label{subsec:clustering_evaluation_metrics}

Evaluating clusters is more complicated than evaluating classification or other supervised machine learning tasks.
Unsupervised tasks can be evaluated using interal or external metrics. While, internal metrics are derived from the algorithm itself e.g. Cluster purity, external metrics are calculated from the comparison with given labels.

Since the in \cref{subsec:so2sat} exaplained dataset contains labels, this section is going to focus on external metrics.
For the here presented metrics we define \(C\) as the ground truth classes and \(K\) as the clustering assignment.
ON this basis we define the following:\\
\(a\) - the number of pairs of elements that are in the same set in \(C\) and in the same set in \(K\)\\
\(b\) - the number of pairs of elements that are in different sets in \(C\) and in different sets in \(K\)\\


\subsubsection{Adjusted Rand Index}
The \gls{ARI} measures the similarity of two assignments. While the raw Rand Index has problems with random assignments, the \gls{ARI} assigns a score close to \(0\)
to random assignments.
The raw Rand Index is mathematically given by \cite{noauthor_23_2020} as
\[RI = \frac{a + b}{C_2^{n_{samples}}}\]
When adjusting this index for random assignments we yield:
\[ARI = \frac{RI - \mathbb{E} [RI]}{\max (RI) - \mathbb{E} [RI]}\]


\subsubsection{V-Measure}
\label{ssec:v_measure}
The V-Measure was introduced by \citeauthor{rosenberg_v-measure_2007} in \cite{rosenberg_v-measure_2007} as an external clustering evaluation metric based on entropy.
The measurement is computed as a combination of Homogenity and Completeness. This allows the measurement to be explained more easily by logging Homogenity and Completeness additionally.
Homogenity measures if only data points of a single class are assigned to a single cluster. Completeness is defined symmetrically to Homogenity by measuring if all datapoints of a single class are assigned to
a single cluster.
When combining these two with \(\beta\) we get the V-Measure \cite{noauthor_23_2020}:
\[v = \frac{(1 + \beta) \times \mathit{Homogenity} \times \mathit{Completeness}}{\beta \times \mathit{Homogenity} + \mathit{Completeness}}\]
Depending on the value for \(\beta\) we put more weights on Homogenity or Completeness. The usual value for \(\beta\) is \(1.0\) which weights both components equally.
The measurement is symmetric and bounded between \(0\) and \(1\), the later being the perfect achievable score.



\begin{figure*}
  \includegraphics[width=0.9\linewidth]{images/strong_scaling_chart.png}
  \caption{Strong scaling behaviour of the Spectral Clustering algorithm. Baseline of 1 node with \(6489s\) excluded for visual purposes.}\label{fig:strong_scaling}
\end{figure*}
