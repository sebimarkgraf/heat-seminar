\section{Implementation}
\label{sec:implementation}

\subsection{Dataset Loading}
\label{subsec:dataset_loading}
The dataset is available as two \gls{HDF5} files, representing a training - validation split.
Both parts of the dataset contain a part of the the Sentinel-1 and Sentinel-2 image patches as well as the labels corresponding to these images.
To load them with \gls{HeAT} the \gls{HDF5} support needs to be installed.
The training subset contains around \(350\,000\) samples.
The implementation can be configured to load the Sentinel-1 or Sentinel-2 dataset and specify a percentage of the dataset which should be loaded.
This enables easier testing and profiling of the scaling behavior.


\subsection{Feature Reshaping}
\label{subsec:feature_reshaping}
When loading the dataset the image patches are available in the shape of \[(num\_samples, channels, width, height)\]
To use these in the clustering we need a feature vector for each sample instead of the nested information. Therefore,
we aim for the shape \[(num\_samples, feature\_dim)\]

Before reshaping, the data is already split into batches of samples onto different nodes. Therefore, when flattening the channels of a sample, no communication should be incurred.
This proved to be more difficult than anticipated. The \gls{numpy} reshape function allows to specify \(-1\) for a dimension that should be automatically calculated.
\gls{HeAT} did not have this option at the beginning of this work.
Additionally, reshape was not optimized for operations that could occur locally and always performed communication even
if it was not necessary in this case. After communicating both of these wishes, the \gls{HeAT} team provided support for the \(-1\) notation and
implemented a fix for the additional communication.
For a workaround during the development time, the implementation used a custom version of \lstinline{ht.flatten} to perform the necessary operation.

\subsection{Normalization}
\label{subsec:normalization}
Due to the channels being in different formats, it can be useful to normalize all channels.
Therefore, channelwise z-score normalization is implemented \cite{wooldridge_introductory_2012}:
\[Z = \frac{X - \mu}{\sigma}\]
Especially when using small datasets, \(\sigma\) could be potentially \(0\) which leads to division by zero errors.
Therefore, the normalization can be deactivated for time profiling.

\subsection{Logging}
\label{subsec:logging}
For the whole project the default Python logging \cite{noauthor_pep_nodate} is used.
All important steps print their beginning and their end on \lstinline{INFO} level.
More specific information, especially about the shapes of the data after transformation is output to \lstinline{DEBUG}.
Per default a console handler is attached that outputs all logging of all nodes to \lstinline{STDOUT}.
\gls{HeAT} does not implement standard library logging and therefore internal steps of the algorithms are not visible on the log files.
For more information about the usage of Python logging refer to \cite{noauthor_pep_nodate}.


\subsection{Experiment Tracking}
\label{subsec:experiment_tracking}
All runs are tracked with WanDB \cite{noauthor_weights_nodate}. Integrating WanDB with the script was difficult due
to the fact that the tracking is not built for distributed systems. Therefore, only the root node should perform
logging to the endpoint.
Thus, only the root node loads the configuration from the disk and broadcasts the current configuration
to all other nodes.
Additionally, all logging methods are wrapped in a decorator, that only allows the root node to execute the function.
Due to the configuration being tracked as well, it is easily possible to sort all runs and filter if they crashed or
compare runs with similar configuration.

One has to be careful when the script crashes due to \gls{MPI} errors. Since \gls{MPI} errors are thrown
below the Python level and the tracking runs on the Python level, there are no signs of an error when looking only at
the tracking. This is caused by the design and cannot be circumvented. It is advisable to check runs regularly on
the cluster or running machine itself.

\subsection{Timing}
\label{subsec:timing}
Only the fitting of the spectral clustering algorithm is timed. This is done by wrapping the fit call with a start
and end time. The prediction of labels for all elements and any plotting is performed after the time measurement.
Every run executes the algorithm multiple times to limit the influence of start up and loading effects.

\subsection{bwUniCluster}
\label{subsec:bw_uni_cluster}
The bwUniCluster 2.0 was commissioned on the 17.03.2020 by the \gls{SCC} \cite{haefner_kit_2020}.
It is part of the framework of the \gls{bwHPC}. In total the cluster has 848 nodes.
The fat and high throughput compute nodes are connected via InifiBand HDR100, which has a throughput of 100 Gbit/s.
The connection to the storage is done via InifiBand EDR.

Scheduling of jobs on the system is done with \gls{SLURM} \cite{yoo_slurm_2003}.
\gls{SLURM} allows to specify the required resources for a job and schedules it for an available time slot.
The specification is mainly done through partitions which group a specific type of node, e.g. fat nodes, multiple nodes, \gls{GPU} nodes.
Depending on the chosen partition the user can configure different requirements regarding the number of nodes, memory and needed time.
This work uses the multiple partition for most tasks and the fat partition for running jobs on single nodes.
